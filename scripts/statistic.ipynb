{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics of FORGE-curated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from dataclasses import field\n",
    "from typing import List, Dict, Union, Optional, Literal\n",
    "from pydantic import BaseModel, RootModel, Field\n",
    "from collections import namedtuple,Counter\n",
    "\n",
    "@dataclass\n",
    "class ProjectInfo:\n",
    "    url: Union[str, int, List, None] = \"n/a\"\n",
    "    commit_id: Union[str, int, List, None] = \"n/a\"\n",
    "    address: Union[str, int, List, None] = \"n/a\"\n",
    "    chain: Union[str, int, List, None] = \"n/a\"\n",
    "    compiler_version: Union[str, List, None] = \"n/a\"\n",
    "    audit_date: Union[str, int, List, None] = \"n/a\"\n",
    "    project_path: Union[str, List, Dict, None] = \"n/a\"\n",
    "\n",
    "    def is_empty(self):\n",
    "        if (self.url == \"n/a\" and self.address == \"n/a\") or (\n",
    "            not self.url and not self.address\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "@dataclass\n",
    "class Finding:\n",
    "    id: Union[str, int] = 0\n",
    "    category: Dict = field(default_factory=dict)\n",
    "    title: str = \"\"\n",
    "    description: str = \"\"\n",
    "    severity: Optional[str] = \"\"\n",
    "    location: Union[str, int, List] = \"\"\n",
    "\n",
    "\n",
    "class Report(BaseModel):\n",
    "    path: str = \"\"\n",
    "    project_info: ProjectInfo = field(default_factory=ProjectInfo)\n",
    "    findings: List[Finding] = field(default_factory=list)\n",
    "\n",
    "    def append_finding(self, finding: Finding):\n",
    "        self.findings.append(finding)\n",
    "\n",
    "\n",
    "class JSONFileProcessor:\n",
    "    def __init__(self, directory: str):\n",
    "        self.directory = directory\n",
    "        self.file_count = 0\n",
    "\n",
    "    def _get_all_json_files(self) -> List[str]:\n",
    "        json_files = []\n",
    "        for root, _, files in os.walk(self.directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\".json\"):\n",
    "                    json_files.append(os.path.join(root, file))\n",
    "                    self.file_count += 1\n",
    "        return json_files\n",
    "\n",
    "    def operate_add(self,results:List,result_type):\n",
    "        res = {}\n",
    "        for field in result_type._fields:\n",
    "            if isinstance(getattr(results[0],field),int):\n",
    "                res[field] = 0\n",
    "            else:\n",
    "                res[field] = []\n",
    "            # res[field] = 0\n",
    "        for result in results:\n",
    "            for field in result._fields:\n",
    "                res[field] += getattr(result, field)\n",
    "\n",
    "        return res\n",
    "    def operate_reduce(self,results:List,result_type):\n",
    "        res = {}\n",
    "        for field in result_type._fields:\n",
    "            res[field] = []\n",
    "        for result in results:\n",
    "            for field in result._fields:\n",
    "                res[field].extend(getattr(result, field))\n",
    "        return res\n",
    "\n",
    "    def process_files(\n",
    "        self, analysis_func=None, modify_func=None\n",
    "    ) -> List[Report]:\n",
    "        results = []\n",
    "        json_files = self._get_all_json_files()\n",
    "        for json_file in json_files:\n",
    "            # print(f\"Processing file: {json_file}\")\n",
    "            with open(json_file, \"r\", encoding=\"utf8\") as f:\n",
    "                data = json.load(f)\n",
    "            report = Report(**data)\n",
    "            if analysis_func:\n",
    "                result = analysis_func(report)\n",
    "                results.append(result)\n",
    "            if modify_func:\n",
    "                modified_report: Report = modify_func(report)\n",
    "                with open(json_file, \"w\", encoding=\"utf8\") as f:\n",
    "                    f.write(modified_report.model_dump_json(indent=4,exclude={'project_info': {'compiler_version'}}))\n",
    "        return results\n",
    "\n",
    "class CWE(BaseModel):\n",
    "    ID: int\n",
    "    Name: str\n",
    "    Description: str = \"\"\n",
    "    Abstraction: Literal[\"Pillar\", \"Class\", \"Base\", \"Variant\", \"Compound\"]\n",
    "    Mapping: Literal[\"Allowed\", \"Allowed-with-Review\", \"Discouraged\", \"Prohibited\"]\n",
    "    Peer: List = Field(default_factory=list)\n",
    "    Parent: List = Field(default_factory=list)\n",
    "    Child: List[int] = Field(default_factory=list)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"CWE-{self.ID}: {self.Name}\"\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "\n",
    "    def add_child(self, child_cwe: \"CWE\"):\n",
    "        self.Child.append(child_cwe)\n",
    "        child_cwe.Parent.append(self)\n",
    "\n",
    "\n",
    "class CWEDatabase(RootModel):\n",
    "    root: Dict[str, CWE]\n",
    "\n",
    "    def get_by_id(self, id: int | str):\n",
    "        name = f\"CWE-{id}\"\n",
    "        return self.root[name]\n",
    "\n",
    "    def get_by_name(self, name: str):\n",
    "        return self.root[name]\n",
    "\n",
    "severity_score_map = {\n",
    "    \"informational\": (0, 0, 0),\n",
    "    \"low\": (0.1, 3.9, 2),\n",
    "    \"medium\": (4.0, 6.9, 5.45),\n",
    "    \"high\": (7.0, 8.9, 7.95),\n",
    "    \"critical\": (9.0, 10.0, 9.5),\n",
    "}\n",
    "\n",
    "class CWEHandler:\n",
    "    def __init__(self, cwe_database: CWEDatabase):\n",
    "        self.db = cwe_database\n",
    "        self.setup_relationships()\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_file(cls, file_path: str) -> \"CWEHandler\":\n",
    "        import json\n",
    "\n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        # CWEDatabase expects a dict where keys are CWE IDs (strings) and values are CWE objects\n",
    "        db = CWEDatabase.model_validate(data)\n",
    "        return cls(db)\n",
    "\n",
    "    def setup_relationships(self):\n",
    "        \"\"\"\n",
    "        Build the parent-child relationships (DAG) based on the 'Child' field (List[int]).\n",
    "        Populates the 'Parent' field (List[CWE]) for each CWE.\n",
    "        \"\"\"\n",
    "        # First pass: Convert any integer IDs in Parent to CWE objects or clear if invalid\n",
    "        for cwe in self.db.root.values():\n",
    "            cleaned_parents = []\n",
    "            for p in cwe.Parent:\n",
    "                if isinstance(p, CWE):\n",
    "                    cleaned_parents.append(p)\n",
    "                elif isinstance(p, int):\n",
    "                    try:\n",
    "                        parent_cwe = self.db.get_by_id(p)\n",
    "                        cleaned_parents.append(parent_cwe)\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "            cwe.Parent = cleaned_parents\n",
    "\n",
    "        # Second pass: Build relationships from Child fields\n",
    "        for cwe_key, cwe in self.db.root.items():\n",
    "            for child_id in cwe.Child:\n",
    "                try:\n",
    "                    child_cwe = self.db.get_by_id(child_id)\n",
    "                    # Add current cwe as parent to the child\n",
    "                    # Check if already exists to avoid duplicates if run multiple times\n",
    "                    if cwe not in child_cwe.Parent:\n",
    "                        child_cwe.Parent.append(cwe)\n",
    "                except KeyError:\n",
    "                    # Child ID might not exist in the database\n",
    "                    continue\n",
    "\n",
    "    def get_cwe(self, cwe_id: Union[int, str]) -> Optional[CWE]:\n",
    "        try:\n",
    "            if isinstance(cwe_id, int):\n",
    "                return self.db.get_by_id(cwe_id)\n",
    "            elif isinstance(cwe_id, str):\n",
    "                if cwe_id.startswith(\"CWE-\"):\n",
    "                    return self.db.root[cwe_id]\n",
    "                else:\n",
    "                    return self.db.get_by_id(int(cwe_id))\n",
    "        except (KeyError, ValueError):\n",
    "            return None\n",
    "\n",
    "    def get_direct_children(self, cwe_id: Union[int, str]) -> List[CWE]:\n",
    "        cwe = self.get_cwe(cwe_id)\n",
    "        if not cwe:\n",
    "            return []\n",
    "        children = []\n",
    "        for child_id in cwe.Child:\n",
    "            child = self.get_cwe(child_id)\n",
    "            if child:\n",
    "                children.append(child)\n",
    "        return children\n",
    "\n",
    "    def get_direct_parents(self, cwe_id: Union[int, str]) -> List[CWE]:\n",
    "        cwe = self.get_cwe(cwe_id)\n",
    "        if not cwe:\n",
    "            return []\n",
    "        return cwe.Parent\n",
    "\n",
    "    def get_all_descendants(self, cwe_id: Union[int, str]) -> List[CWE]:\n",
    "        cwe = self.get_cwe(cwe_id)\n",
    "        if not cwe:\n",
    "            return []\n",
    "        descendants = set()\n",
    "        queue = [cwe]\n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            for child_id in current.Child:\n",
    "                child = self.get_cwe(child_id)\n",
    "                if child and child not in descendants:\n",
    "                    descendants.add(child)\n",
    "                    queue.append(child)\n",
    "        return list(descendants)\n",
    "\n",
    "    def get_root_parents(self, cwe_id: Union[int, str]) -> List[CWE]:\n",
    "        \"\"\"\n",
    "        Recursively find the most root parents (Pillars).\n",
    "        \"\"\"\n",
    "        cwe = self.get_cwe(cwe_id)\n",
    "        if not cwe:\n",
    "            return []\n",
    "\n",
    "        roots = set()\n",
    "        queue = [cwe]\n",
    "        visited = set()\n",
    "\n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            if current in visited:\n",
    "                continue\n",
    "            visited.add(current)\n",
    "\n",
    "            # If it's a Pillar, it's a root\n",
    "            if current.Abstraction == \"Pillar\":\n",
    "                roots.add(current)\n",
    "                continue  # Stop traversing up from a Pillar? Usually yes.\n",
    "\n",
    "            # If no parents, it's a root\n",
    "            if not current.Parent:\n",
    "                roots.add(current)\n",
    "                continue\n",
    "\n",
    "            # Otherwise, traverse up\n",
    "            for parent in current.Parent:\n",
    "                queue.append(parent)\n",
    "\n",
    "        return list(roots)\n",
    "\n",
    "    def is_related(self, id1: Union[int, str], id2: Union[int, str]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if there is any inheritance relationship between two CWEs.\n",
    "        Returns True if id1 is ancestor of id2 OR id2 is ancestor of id1.\n",
    "        \"\"\"\n",
    "        cwe1 = self.get_cwe(id1)\n",
    "        cwe2 = self.get_cwe(id2)\n",
    "\n",
    "        if not cwe1 or not cwe2:\n",
    "            return False\n",
    "\n",
    "        if cwe1 == cwe2:\n",
    "            return True\n",
    "\n",
    "        # Check if cwe1 is ancestor of cwe2\n",
    "        if self._is_ancestor(cwe1, cwe2):\n",
    "            return True\n",
    "\n",
    "        # Check if cwe2 is ancestor of cwe1\n",
    "        if self._is_ancestor(cwe2, cwe1):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _is_ancestor(self, ancestor: CWE, descendant: CWE) -> bool:\n",
    "        # BFS up from descendant to find ancestor\n",
    "        queue = [descendant]\n",
    "        visited = set()\n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            if current in visited:\n",
    "                continue\n",
    "            visited.add(current)\n",
    "\n",
    "            if current == ancestor:\n",
    "                return True\n",
    "\n",
    "            for parent in current.Parent:\n",
    "                queue.append(parent)\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count findings & projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_files': 209, 'total_projects': 254, 'total_findings': 2556}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINDING_PATH = \"../dataset-curated/findings\"\n",
    "VERIFY_PATH = \"../dataset-curated/manual_verification\"\n",
    "CONTRACTS_PATH = \"../dataset-curated/contracts\"\n",
    "CONTRACTS_RAW_PATH = \"../dataset-curated/contracts-raw\"\n",
    "VFP_PATH =\"../flatten/vfp\"\n",
    "VFP_VULN_PATH =\"../flatten/vfp-vuln\"\n",
    "\n",
    "\n",
    "Finding = namedtuple(\"Finding\", [\"total_files\",\"total_projects\",\"total_findings\"])\n",
    "\n",
    "def count_finding(report: Report):\n",
    "    if isinstance(report.project_info.url,list):\n",
    "        # print(report.path)\n",
    "        project_count = len(report.project_info.url)\n",
    "    else:\n",
    "        project_count = 1\n",
    "    result = Finding(total_findings=len(report.findings),total_files=1,total_projects=project_count)\n",
    "    return result\n",
    "\n",
    "processor = JSONFileProcessor(FINDING_PATH)\n",
    "results = processor.process_files(analysis_func=count_finding)\n",
    "res = processor.operate_add(results,Finding)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check & count contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_files': 209,\n",
       " 'total_projects': 210,\n",
       " 'valid_projects': 210,\n",
       " 'solidity_files': 28925,\n",
       " 'lines_of_code': 4724389}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "Result = namedtuple(\n",
    "    \"Result\",\n",
    "    [\n",
    "        \"total_files\",\n",
    "        \"total_projects\", \n",
    "        \"valid_projects\",\n",
    "        \"solidity_files\",\n",
    "        \"lines_of_code\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "_processed_projects = set()\n",
    "\n",
    "def check_projects(report: Report):\n",
    "    def count_lines(filepath: Path):\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf8\", errors='ignore') as rust_file:\n",
    "                lines = len(rust_file.readlines())\n",
    "                return lines\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening file {filepath}: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    if report.project_info.is_empty():\n",
    "        valid_projects = 0\n",
    "        solidity_files = 0\n",
    "        lines_of_code = 0\n",
    "        total_projects = 0\n",
    "        print(f\"Empty project info in {report.path}\")\n",
    "    else:\n",
    "        valid_projects = 0\n",
    "        solidity_files = 0\n",
    "        lines_of_code = 0\n",
    "        total_projects = 0\n",
    "        \n",
    "        project_paths = []\n",
    "        if isinstance(report.project_info.project_path, dict):\n",
    "            project_paths = list(report.project_info.project_path.values())\n",
    "        elif report.project_info.project_path and report.project_info.project_path != \"n/a\":\n",
    "            project_paths = [report.project_info.project_path]\n",
    "        \n",
    "        for v in project_paths:\n",
    "            if not v or v == \"n/a\":\n",
    "                continue\n",
    "                \n",
    "            # \"dataset-curated/\"\n",
    "            \n",
    "            project_path = Path(\"../\") / Path(v)\n",
    "            \n",
    "            project_key = str(project_path.resolve()) if project_path.exists() else str(project_path)\n",
    "            if project_key in _processed_projects:\n",
    "                total_projects += 1\n",
    "                continue\n",
    "            \n",
    "            _processed_projects.add(project_key)\n",
    "            total_projects += 1\n",
    "            \n",
    "\n",
    "            if project_path.exists():\n",
    "                valid_projects += 1\n",
    "                solidity_file_paths = [p for p in project_path.glob(\"**/*.sol\") if p.is_file()]\n",
    "                solidity_files += len(solidity_file_paths)\n",
    "                for path in solidity_file_paths:\n",
    "                    lines_of_code += count_lines(path)\n",
    "    \n",
    "    return Result(\n",
    "        total_files=1,\n",
    "        total_projects=total_projects,\n",
    "        valid_projects=valid_projects,\n",
    "        solidity_files=solidity_files,\n",
    "        lines_of_code=lines_of_code,\n",
    "    )\n",
    "\n",
    "\n",
    "_processed_projects.clear()\n",
    "\n",
    "processor = JSONFileProcessor(FINDING_PATH)\n",
    "results = processor.process_files(analysis_func=check_projects)\n",
    "res = processor.operate_add(results, Result)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 209\n",
      "Total projects: 210\n",
      "Valid projects: 210\n",
      "Total solidity files: 28925\n",
      "Total lines of code: 4724389\n",
      "Average lines of code: 22497.090476190475\n",
      "Average files per project: 137.73809523809524\n"
     ]
    }
   ],
   "source": [
    "average_lines_of_code = res[\"lines_of_code\"] / res[\"valid_projects\"]\n",
    "average_files_per_project = res[\"solidity_files\"] / res[\"valid_projects\"]\n",
    "print(f\"Total files: {res['total_files']}\")\n",
    "print(f\"Total projects: {res['total_projects']}\")\n",
    "print(f\"Valid projects: {res['valid_projects']}\")\n",
    "print(f\"Total solidity files: {res['solidity_files']}\")\n",
    "print(f\"Total lines of code: {res['lines_of_code']}\")\n",
    "print(f\"Average lines of code: {average_lines_of_code}\")\n",
    "print(f\"Average files per project: {average_files_per_project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Severity statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'na': 93,\n",
       " 'informational': 908,\n",
       " 'low': 794,\n",
       " 'medium': 439,\n",
       " 'high': 254,\n",
       " 'critical': 68}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "severity_category = [\"na\",\"informational\",\"low\",\"medium\",\"high\",\"critical\"]\n",
    "\n",
    "Severity = namedtuple(\"Category\",severity_category)\n",
    "def count_severity(report: Report):\n",
    "    findings = report.findings\n",
    "    severity_dict = {\"na\": 0, \"informational\": 0, \"low\": 0, \"medium\": 0, \"high\": 0, \"critical\": 0}\n",
    "    for finding in findings:\n",
    "        severity = finding.severity\n",
    "        if severity:\n",
    "            severity = finding.severity.lower().strip()\n",
    "        if severity == \"info\" or severity == \"warning\" or severity == \"note/information\" or severity == \"gas\" or severity == \"note\":\n",
    "            severity = \"informational\"\n",
    "        if severity == None or severity == \"\" or severity == \"n/a\":\n",
    "            severity = \"na\"\n",
    "        \n",
    "        \n",
    "        if severity not in severity_category:\n",
    "            print(f\"Unknown severity: {severity}\")\n",
    "            continue\n",
    "        severity_dict[severity] = severity_dict.get(severity, 0) + 1\n",
    "        # dict to namedtuple\n",
    "    return Severity(**severity_dict)\n",
    "\n",
    "processor = JSONFileProcessor(FINDING_PATH)\n",
    "results = processor.process_files(analysis_func=count_severity)\n",
    "res = processor.operate_add(results, Severity)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics of Vulnerability-File Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vulnerability-File Pairs: 654\n",
      "Total Vulnerability-File Pairs with Medium, High and Critical Severity: 319\n"
     ]
    }
   ],
   "source": [
    "vp_files = Path(VFP_PATH).glob(\"*.json\")\n",
    "vp_vuln_files = Path(VFP_VULN_PATH).glob(\"*.json\")\n",
    "\n",
    "vp_count = len(list(vp_files))\n",
    "vp_vuln_count = len(list(vp_vuln_files))\n",
    "print(f\"Total Vulnerability-File Pairs: {vp_count}\")\n",
    "print(f\"Total Vulnerability-File Pairs with Medium, High and Critical Severity: {vp_vuln_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
