{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69c084c4",
   "metadata": {},
   "source": [
    "## Validate Source Code Existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2dd990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-11 15:34:17.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mNumber of finding files: 209\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:17.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mNumber of manual verification files: 0\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:17.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mNumber of contract directories: 209\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "from loguru import logger\n",
    "\n",
    "FINDING_PATH = \"../dataset-curated/findings\"\n",
    "VERIFY_PATH = \"../dataset-curated/manual_verification\"\n",
    "CONTRACTS_PATH = \"../dataset-curated/contracts\"\n",
    "CONTRACTS_RAW_PATH = \"../dataset-curated/contracts-raw\"\n",
    "\n",
    "\n",
    "finding_path = Path(FINDING_PATH).glob(\"*.json\")\n",
    "verify_path = Path(VERIFY_PATH).glob(\"*.json\")\n",
    "\n",
    "\n",
    "# del finding.json that in manual_verification\n",
    "# then delete contracts/finding.pdf-source\n",
    "\n",
    "for verify_file in verify_path:\n",
    "    finding_file = Path(FINDING_PATH) / verify_file.name\n",
    "    if finding_file.exists():\n",
    "        # finding_file.unlink()\n",
    "        logger.info(f\"Deleted finding file: {finding_file}\")\n",
    "    else:\n",
    "        # logger.warning(f\"Finding file not found for deletion: {finding_file}\")\n",
    "        pass\n",
    "\n",
    "    # Delete corresponding source code directory if exists\n",
    "    source_dir_pdf = Path(CONTRACTS_PATH) / f\"{verify_file.stem}.pdf-source\"\n",
    "    source_dir_md = Path(CONTRACTS_PATH) / f\"{verify_file.stem}.md-source\"\n",
    "    if source_dir_pdf.exists() and source_dir_pdf.is_dir():\n",
    "        # shutil.rmtree(source_dir_pdf)\n",
    "        logger.info(f\"Deleted source code directory: {source_dir_pdf}\")\n",
    "    else:\n",
    "        # logger.warning(f\"Source code directory not found for deletion: {source_dir_pdf}\")\n",
    "        pass\n",
    "    if source_dir_md.exists() and source_dir_md.is_dir():\n",
    "        # shutil.rmtree(source_dir_md)\n",
    "        logger.info(f\"Deleted source code directory: {source_dir_md}\")\n",
    "    else:\n",
    "        # logger.warning(f\"Source code directory not found for deletion: {source_dir_md}\")\n",
    "        pass\n",
    "\n",
    "# print file number in \"findings\" and \"manual_verification\" and dir number in \"contracts\"\n",
    "finding_files = list(finding_path)\n",
    "verify_files = list(Path(VERIFY_PATH).glob(\"*.json\"))\n",
    "contracts_dirs = [d for d in Path(CONTRACTS_PATH).iterdir() if d.is_dir()]\n",
    "\n",
    "for finding_file in finding_files:\n",
    "    if str(finding_file.stem)+\".md-source\"in [d.name for d in contracts_dirs] or str(finding_file.stem)+\".pdf-source\" in [d.name for d in contracts_dirs]:\n",
    "        # logger.info(f\"Finding file has corresponding source code: {finding_file}\")\n",
    "        continue\n",
    "    else:\n",
    "        logger.warning(f\"Finding file does not have corresponding source code: {finding_file}\")\n",
    "\n",
    "logger.info(f\"Number of finding files: {len(finding_files)}\")\n",
    "logger.info(f\"Number of manual verification files: {len(verify_files)}\")\n",
    "logger.info(f\"Number of contract directories: {len(contracts_dirs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4017966e",
   "metadata": {},
   "source": [
    "## Unify Finding Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2447a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "\n",
    "finding_path = Path(FINDING_PATH).glob(\"*.json\")\n",
    "\n",
    "severity_mapping = {\n",
    "    \"critical\": \"Critical\",\n",
    "    \"high\": \"High\",\n",
    "    \"high risk\": \"High\",\n",
    "    \"medium\": \"Medium\",\n",
    "    \"low\": \"Low\",\n",
    "    \"low risk\": \"Low\",\n",
    "    \"informational\": \"Informational\",\n",
    "    \"major\": \"Critical\",\n",
    "    \"minor\": \"Low\",\n",
    "    \"info\": \"Informational\",\n",
    "    \"note\": \"Informational\",\n",
    "    \"warning\": \"Informational\",\n",
    "    \"gas\": \"Informational\",\n",
    "    \"gas optimization\": \"Informational\",\n",
    "    \"indeterminate\": \"Informational\",\n",
    "    \"undetermined\": \"Informational\",\n",
    "    \"note/information\": \"Informational\",\n",
    "    \"non-critical\": \"Informational\",\n",
    "}\n",
    "\n",
    "for finding_file in finding_path:\n",
    "    with open(finding_file, \"r\") as f:\n",
    "        finding_dict = json.load(f)\n",
    "    if finding_dict.get(\"project_info\",{}).get(\"project_path\") ==\"n/a\":\n",
    "        logger.warning(f\"Finding missing project_path: {finding_file}\")\n",
    "    for finding in finding_dict[\"findings\"]:\n",
    "        # print(finding[\"id\"], finding[\"severity\"])\n",
    "        if \"severity\" not in finding:\n",
    "            logger.warning(\n",
    "                f\"Finding missing severity: {finding_file} - {finding['id']}\"\n",
    "            )\n",
    "        if finding[\"severity\"] is None:\n",
    "            # logger.warning(\n",
    "            #     f\"Finding has null severity: {finding_file} - {finding['id']}\"\n",
    "            # )\n",
    "            continue\n",
    "        if finding[\"severity\"].lower() in severity_mapping:\n",
    "            finding[\"severity\"] = severity_mapping[finding[\"severity\"].lower()]\n",
    "        else:\n",
    "            logger.warning(\n",
    "                f\"Unknown severity level: {finding_file} - {finding['id']} - {finding['severity']}\"\n",
    "            )\n",
    "    with open(finding_file, \"w\") as f:\n",
    "        json.dump(finding_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f16df3a",
   "metadata": {},
   "source": [
    "## Validate File Existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d91d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-11 15:34:21.829\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/cantina_uniswap_nov2025.pdf-source/protocol-fees/src/V3FeeAdapter.sol - 7 - protocol-fees/src/V3FeeAdapter.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.831\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/ChainSecurity_MellowFinance_Multivault_Audit.pdf-source/src/vaults/ERC4626Vault.sol - 25 - src/vaults/ERC4626Vault.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.832\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/2025-01-bacon-labs-bunniv2-securityreview.pdf-source/95f4270ad4447e96044973580afda9176730e7c8/biddog/src/lib/AmAmmPayload.sol - 10 - 95f4270ad4447e96044973580afda9176730e7c8/biddog/src/lib/AmAmmPayload.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.833\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/2025-01-bacon-labs-bunniv2-securityreview.pdf-source/95f4270ad4447e96044973580afda9176730e7c8/biddog/src/lib/BunniHookLogic.sol - 10 - 95f4270ad4447e96044973580afda9176730e7c8/biddog/src/lib/BunniHookLogic.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.834\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/2025-01-bacon-labs-bunniv2-securityreview.pdf-source/7faae4718eecda1b33dc3abd894431ed2d16c929/bunni-v2/src/lib/LibUniformDistribution.sol - 20 - 7faae4718eecda1b33dc3abd894431ed2d16c929/bunni-v2/src/lib/LibUniformDistribution.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.837\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/2025.07.18 - Final - Notional Exponent Audit Report.pdf-source/7e0abc3e118db0abb20c7521c6f53f1762fdf562/notional-v4/src/withdraws/WithdrawalRequestManager.sol - 0 - 7e0abc3e118db0abb20c7521c6f53f1762fdf562/notional-v4/src/withdraws/WithdrawalRequestManager.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.837\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/2025.07.18 - Final - Notional Exponent Audit Report.pdf-source/7e0abc3e118db0abb20c7521c6f53f1762fdf562/notional-v4/src/withdraws/PirexETH.sol - 2 - 7e0abc3e118db0abb20c7521c6f53f1762fdf562/notional-v4/src/withdraws/PirexETH.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.844\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/2025-03-offchain-custom-fee-erc20-bridge-securityreview.pdf-source/orbit-actions/contracts/parent-chain/contract-upgrades/NitroContracts2Point1Point3UpgradeAction.sol - 0 - orbit-actions/contracts/parent-chain/contract-upgrades/NitroContracts2Point1Point3UpgradeAction.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.846\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/ackee-blockchain-lido-triggerable-withdrawals-report.pdf-source/core/contracts/0.8.9/oracle/ValidatorExitBusOracle.sol - 9 - core/contracts/0.8.9/oracle/ValidatorExitBusOracle.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.847\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/cantina_steakhouse_nov2025.pdf-source/box/src/BoxFactory.sol - 3 - box/src/BoxFactory.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.849\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Enjoyoors EVM Vaults Security Audit Report.pdf-source/EnjoyoorsWithdrawalApprover/contracts/vault/EnjoypoorsWithdrawalApprover.sol - 5 - EnjoyoorsWithdrawalApprover/contracts/vault/EnjoypoorsWithdrawalApprover.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.849\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Enjoyoors EVM Vaults Security Audit Report.pdf-source/EnjoyoorsWithdrawalApprover/contracts/vault/EnjoypoorsWithdrawalApprover.sol - 6 - EnjoyoorsWithdrawalApprover/contracts/vault/EnjoypoorsWithdrawalApprover.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.852\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/ChainSecurity_StudioV_Neulock_Audit.pdf-source/contracts/old/MetadataV2.sol - 20 - contracts/old/MetadataV2.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.853\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/ChainSecurity_StudioV_Neulock_Audit.pdf-source/contracts/old/NeuV2.sol - 21 - contracts/old/NeuV2.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.854\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Frax0 Mesh - Zellic Audit Report.pdf-source/scripts/ops/V110/ethereum/UpgradeAdapterEthereum.s.sol - 5 - scripts/ops/V110/ethereum/UpgradeAdapterEthereum.s.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.856\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Jovay Rollup Contracts Audit.md-source/jovay-contracts/rollup_contracts/contracts/L1/core/L1GasOracle.sol - 33 - jovay-contracts/rollup_contracts/contracts/L1/core/L1GasOracle.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.859\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Across Protocol OFT Integration Differential Audit.md-source/contracts/interfaces/SpokePoolInterface.sol - 30 - contracts/interfaces/SpokePoolInterface.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.861\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/cantina_rocketpool_jun2025.pdf-source/rocketpool/contracts/contract/dao/node/RocketDAONodeTrustedSettingsMembers.sol - 38 - rocketpool/contracts/contract/dao/node/RocketDAONodeTrustedSettingsMembers.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.863\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/2025-04-reserve-folio-solidity-securityreview.pdf-source/reserve-index-dtf/contracts/folio/Folio.sol - 2 - reserve-index-dtf/contracts/folio/Folio.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.864\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Sponsored Periphery Audit.md-source/758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/periphery/mintburn/ArbitraryEVMFlowExecutor.sol - 5 - 758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/periphery/mintburn/ArbitraryEVMFlowExecutor.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.864\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Sponsored Periphery Audit.md-source/758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/handlers/HyperliquidDepositHandler.sol - 6 - 758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/handlers/HyperliquidDepositHandler.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.865\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Sponsored Periphery Audit.md-source/758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/handlers/PermissionedMulticallHandler.sol - 7 - 758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/handlers/PermissionedMulticallHandler.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.865\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Sponsored Periphery Audit.md-source/758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/periphery/mintburn/ArbitraryEVMFlowExecutor.sol - 8 - 758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/periphery/mintburn/ArbitraryEVMFlowExecutor.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.866\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Sponsored Periphery Audit.md-source/758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/periphery/mintburn/SwapHandler.sol - 9 - 758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/periphery/mintburn/SwapHandler.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.868\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Sponsored Periphery Audit.md-source/758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/periphery/mintburn/HyperCoreFlowExecutor.sol - 30 - 758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/periphery/mintburn/HyperCoreFlowExecutor.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.871\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Sponsored Periphery Audit.md-source/758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/periphery/mintburn/HyperCoreFlowExecutor.sol - 31 - 758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/periphery/mintburn/HyperCoreFlowExecutor.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.874\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Sponsored Periphery Audit.md-source/758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/periphery/mintburn/HyperCoreFlowExecutor.sol - 32 - 758570a9ace4a3c2335d2628342f71a33fbbc908/contracts/periphery/mintburn/HyperCoreFlowExecutor.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.876\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Jovay Sequencer System Contracts Audit.md-source/jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/permission_control.sol - 15 - jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/permission_control.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.876\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Jovay Sequencer System Contracts Audit.md-source/jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/sys_staking.sol - 15 - jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/sys_staking.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.877\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Jovay Sequencer System Contracts Audit.md-source/jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/sys_chaincfg.sol - 15 - jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/sys_chaincfg.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.877\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Jovay Sequencer System Contracts Audit.md-source/jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/permission_control.sol - 16 - jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/permission_control.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.877\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Jovay Sequencer System Contracts Audit.md-source/jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/sys_staking.sol - 17 - jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/sys_staking.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.878\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Jovay Sequencer System Contracts Audit.md-source/jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/sys_chaincfg.sol - 17 - jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/sys_chaincfg.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.878\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Jovay Sequencer System Contracts Audit.md-source/jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/sys_staking.sol - 18 - jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/sys_staking.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.879\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Jovay Sequencer System Contracts Audit.md-source/jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/permission_control.sol - 19 - jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/permission_control.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.881\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Jovay Sequencer System Contracts Audit.md-source/jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/sys_staking.sol - 19 - jovay-contracts/sequencer_contracts/aldaba-ng/sys_contract/artifact_src/solidity/sys_staking.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.883\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/cantina_eco_pr28_pr38_aug2025.pdf-source/833b84f8/permit3/src/MultiTokenPermit.sol - 2 - 833b84f8/permit3/src/MultiTokenPermit.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.884\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/cantina_eco_pr28_pr38_aug2025.pdf-source/833b84f8/permit3/src/MultiTokenPermit.sol - 4 - 833b84f8/permit3/src/MultiTokenPermit.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.886\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/ChainSecurity_Sky_RatesConverter_Audit.pdf-source/src/Conv.sol - 0 - src/Conv.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.887\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/OIF Contracts Diff Audit.md-source/oif-contracts/src/integrations/ChainMap.sol - 2 - oif-contracts/src/integrations/ChainMap.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.892\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Paxos Labs - Zenith Audit Report.pdf-source/src/helper/one-to-one-queue/OneToOneQueue.sol - 0 - src/helper/one-to-one-queue/OneToOneQueue.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.893\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Paxos Labs - Zenith Audit Report.pdf-source/src/helper/one-to-one-queue/OneToOneQueue.sol - 1 - src/helper/one-to-one-queue/OneToOneQueue.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.893\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Paxos Labs - Zenith Audit Report.pdf-source/src/helper/one-to-one-queue/OneToOneQueue.sol - 2 - src/helper/one-to-one-queue/OneToOneQueue.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.894\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Paxos Labs - Zenith Audit Report.pdf-source/src/helper/one-to-one-queue/OneToOneQueue.sol - 3 - src/helper/one-to-one-queue/OneToOneQueue.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.894\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Paxos Labs - Zenith Audit Report.pdf-source/src/helper/DistributorCodeDepositor.sol - 4 - src/helper/DistributorCodeDepositor.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.896\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/OpenZeppelin Uniswap Hooks v1.1.0 RC 1 Audit.md-source/src/interfaces/IHookEvents.sol - 15 - src/interfaces/IHookEvents.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.896\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/OpenZeppelin Uniswap Hooks v1.1.0 RC 1 Audit.md-source/src/general/LimitOrderHook.sol - 15 - src/general/LimitOrderHook.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.897\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/cantina_managed_sky_vote_delegate_april2025.pdf-source/src/VoteDelegate.sol - 0 - src/VoteDelegate.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.899\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Parallel Protocol - Zenith Audit Report.pdf-source/contracts/fees/MainFeeDistributor.sol - 15 - contracts/fees/MainFeeDistributor.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.902\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/1inch Cross-Chain Swap V1.1.0 Audit.md-source/contracts/libraries/ImmutablesLib.sol - 0 - contracts/libraries/ImmutablesLib.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.905\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Lido V3 _ Consensys Diligence.md-source/core/contracts/common/NodeOperatorFee.sol - 14 - core/contracts/common/NodeOperatorFee.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.906\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/f(x) v2 Audit.md-source/fx-protocol-contracts/contracts/fund/AaveV3Strategy.sol - 17 - fx-protocol-contracts/contracts/fund/AaveV3Strategy.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.908\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/cantina_tadle_oct2025.pdf-source/v3-sandbox-audit_1/src/relayers/monad_testnet/airdrop/main.sol - 1 - v3-sandbox-audit_1/src/relayers/monad_testnet/airdrop/main.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.909\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/cantina_tadle_oct2025.pdf-source/v3-sandbox-audit_1/src/relayers/monad_testnet/airdrop/main.sol - 10 - v3-sandbox-audit_1/src/relayers/monad_testnet/airdrop/main.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.912\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Distributor Diff Audit.md-source/src/ZkMerkleDistributor.sol - 0 - src/ZkMerkleDistributor.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.912\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/Distributor Diff Audit.md-source/src/ZkTokenV1.sol - 0 - src/ZkTokenV1.sol\u001b[0m\n",
      "\u001b[32m2026-02-11 15:34:21.914\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[33m\u001b[1mFile in finding does not exist: ../dataset-curated/contracts/ackee-blockchain-vfat-sickle-report.pdf-source/sickle-public/contracts/SickleMultisig.sol - 18 - sickle-public/contracts/SickleMultisig.sol\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 3041, Invalid files: 57\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "\n",
    "finding_path = Path(FINDING_PATH).glob(\"*.json\")\n",
    "\n",
    "invalid_file_count = 0\n",
    "total_file_count = 0\n",
    "for finding_file in finding_path:\n",
    "    with open(finding_file, \"r\") as f:\n",
    "        finding_dict = json.load(f)\n",
    "    if finding_dict.get(\"project_info\",{}).get(\"project_path\") ==\"n/a\":\n",
    "        logger.warning(f\"Finding missing project_path: {finding_file}\")\n",
    "    dataset_root_path = Path(list(finding_dict.get(\"project_info\",{}).get(\"project_path\",{}).values())[0])\n",
    "    for finding in finding_dict[\"findings\"]:\n",
    "        if \"files\" not in finding:\n",
    "            logger.warning(\n",
    "                f\"Finding missing files field: {finding_file} - {finding['id']}\"\n",
    "            )\n",
    "            continue\n",
    "        files_list = finding[\"files\"]\n",
    "        \n",
    "        for ff in files_list:\n",
    "            rel_path  = Path(\"../\") /dataset_root_path / ff\n",
    "            if not (rel_path).exists() and rel_path.suffix in [\".sol\"]:\n",
    "                # print(dataset_root_path / ff)\n",
    "                invalid_file_count += 1\n",
    "                logger.warning(\n",
    "                    f\"File in finding does not exist: {rel_path} - {finding['id']} - {ff}\"\n",
    "                )\n",
    "            total_file_count += 1\n",
    "print(f\"Total files: {total_file_count}, Invalid files: {invalid_file_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc584b",
   "metadata": {},
   "source": [
    "## Generate Vulnerability-File Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "024c618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union, Optional, Literal, Tuple,Set\n",
    "from pydantic import BaseModel, RootModel, Field\n",
    "from dataclasses import field, dataclass\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class ProjectInfo:\n",
    "    url: Union[str, int, List, None] = \"n/a\"\n",
    "    commit_id: Union[str, int, List, None] = \"n/a\"\n",
    "    address: Union[str, int, List, None] = \"n/a\"\n",
    "    chain: Union[str, int, List, None] = \"n/a\"\n",
    "    compiler_version: Union[str, List, None] = \"n/a\"\n",
    "    audit_date: Union[str, int, List, None] = \"n/a\"\n",
    "    project_path: Union[str, List, Dict, None] = \"n/a\"\n",
    "\n",
    "    def is_empty(self):\n",
    "        if (self.url == \"n/a\" and self.address == \"n/a\") or (\n",
    "            not self.url and not self.address\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "    def __hash__(self):\n",
    "        return hash((self.url, self.commit_id, self.address, self.chain, self.compiler_version, self.audit_date, str(self.project_path)))\n",
    "    \n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Finding:\n",
    "    id: Union[str, int] = 0\n",
    "    category: Dict = field(default_factory=dict)\n",
    "    title: str = \"\"\n",
    "    description: str = \"\"\n",
    "    severity: Optional[str] = \"\"\n",
    "    location: Union[str, int, List] = \"\"\n",
    "    files: List[str] = field(default_factory=list)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.id, self.category, self.title, self.description, self.severity, self.location, tuple(self.files)))\n",
    "\n",
    "class Report(BaseModel):\n",
    "    path: str = \"\"\n",
    "    project_info: ProjectInfo = field(default_factory=ProjectInfo)\n",
    "    findings: List[Finding] = field(default_factory=list)\n",
    "\n",
    "    def append_finding(self, finding: Finding):\n",
    "        self.findings.append(finding)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.path, self.project_info, tuple(self.findings)))\n",
    "\n",
    "class VulnerabilityFilePair(BaseModel):\n",
    "    vfp_id: str = \"\" # Unique ID for the VulnerabilityFilePair, e.g., 'vfp_00001'\n",
    "    project_name: str = \"\"\n",
    "    findings: List[Finding] = Field(default_factory=list)\n",
    "    affected_files: Dict[str, str] = Field(default_factory=dict)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.vfp_id, self.project_name,tuple(self.findings), tuple(self.affected_files)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd977d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "ONLY_VULN = False\n",
    "\n",
    "\n",
    "def load_report(report_path: str) -> Report:\n",
    "    \"\"\"Load a report from a JSON file.\"\"\"\n",
    "    import json\n",
    "\n",
    "    with open(report_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return Report.model_validate(data)\n",
    "\n",
    "\n",
    "def process_reports(\n",
    "    input_path: Union[str, Path],\n",
    "    dataset_path=\"../\",\n",
    "    output_vfp_dir=\"../dataset-curated/vfp\",\n",
    ") -> List[VulnerabilityFilePair]:\n",
    "    \"\"\"\n",
    "    Process reports from a file or directory and generate VulnPairs.\n",
    "    Aggregates findings based on overlapping files.\n",
    "    Saves each VFP to output_vfp_dir as {vfp_id}.json\n",
    "    \"\"\"\n",
    "    input_path = Path(input_path)\n",
    "    output_vfp_dir = Path(output_vfp_dir)\n",
    "    output_vfp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Convert dataset_path to absolute path for consistent path operations\n",
    "    dataset_path_abs = Path(dataset_path).resolve()\n",
    "\n",
    "    reports = []\n",
    "\n",
    "    if input_path.is_file():\n",
    "        if input_path.suffix == \".json\":\n",
    "            reports.append(load_report(str(input_path)))\n",
    "    elif input_path.is_dir():\n",
    "        for file_path in input_path.glob(\"*.json\"):\n",
    "            reports.append(load_report(str(file_path)))\n",
    "\n",
    "    vuln_pairs = []\n",
    "    global_vfp_counter = 1  # Global counter for VFP IDs\n",
    "\n",
    "    for report in reports:\n",
    "\n",
    "        file_to_findings: Dict[str, Set[int]] = {}\n",
    "        finding_files: Dict[int, Set[str]] = {}\n",
    "\n",
    "        project_root = Path(list(report.project_info.project_path.values())[0])\n",
    "\n",
    "        valid_findings = []\n",
    "        for i, finding in enumerate(report.findings):\n",
    "\n",
    "            # Filter out findings with no files or no category\n",
    "            if not finding.files:\n",
    "                continue\n",
    "\n",
    "            if ONLY_VULN:\n",
    "                if finding.severity is None or finding.severity.lower() not in [\n",
    "                    \"critical\",\n",
    "                    \"high\",\n",
    "                    \"medium\",\n",
    "                ]:\n",
    "                    continue\n",
    "\n",
    "            # Resolve file paths\n",
    "            resolved_files = set()\n",
    "            for f_rel in finding.files:\n",
    "                # Sometimes paths might be absolute or relative\n",
    "                # Assuming relative to project_path as per instruction\n",
    "                try:\n",
    "                    abs_path = (Path(dataset_path) / project_root / f_rel).resolve()\n",
    "                    # get file's extension\n",
    "                    ext = abs_path.suffix.lower()\n",
    "                    if ext in {\".md\", \".pdf.json\"}:\n",
    "                        continue\n",
    "                    if ext not in {\".rs\", \".ts\", \".sol\", \".toml\", \".sh\", \".json\"}:\n",
    "                        # print(f\"Resolved file: {abs_path} with extension {ext}\")\n",
    "                        continue\n",
    "\n",
    "                    # if not os.path.exists(abs_path):\n",
    "                    #     print(f\"Warning: Resolved file does not exist: {abs_path}\")\n",
    "                    if os.path.exists(abs_path):\n",
    "\n",
    "                        resolved_files.add(str(abs_path))\n",
    "\n",
    "                except Exception:\n",
    "                    print(\n",
    "                        f\"Warning: Could not resolve file path {f_rel} in project {project_root}\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            if not resolved_files:\n",
    "                continue\n",
    "\n",
    "            valid_findings.append(finding)\n",
    "            f_idx = len(valid_findings) - 1\n",
    "            finding_files[f_idx] = resolved_files\n",
    "\n",
    "            for f_path in resolved_files:\n",
    "                if f_path not in file_to_findings:\n",
    "                    file_to_findings[f_path] = set()\n",
    "                file_to_findings[f_path].add(f_idx)\n",
    "\n",
    "        # 2. Build connected components of findings\n",
    "        # Two findings are connected if they share a file\n",
    "        # We can use Union-Find or BFS/DFS\n",
    "\n",
    "        num_findings = len(valid_findings)\n",
    "        visited_findings = [False] * num_findings\n",
    "\n",
    "        for i in range(num_findings):\n",
    "            # print(f\"Finding {i}: visited={visited_findings[i]}\")  # Debugging line\n",
    "            if visited_findings[i]:\n",
    "                continue\n",
    "\n",
    "            # Start a new component\n",
    "            component_findings_indices = set()\n",
    "            queue = [i]\n",
    "            visited_findings[i] = True\n",
    "\n",
    "            while queue:\n",
    "                # Debugging line\n",
    "                curr_idx = queue.pop(0)\n",
    "                component_findings_indices.add(curr_idx)\n",
    "\n",
    "                # Get all files for this finding\n",
    "                files = finding_files[curr_idx]\n",
    "\n",
    "                # For each file, get all other findings that touch it\n",
    "                for f_path in files:\n",
    "                    linked_findings = file_to_findings[f_path]\n",
    "                    for linked_idx in linked_findings:\n",
    "                        if not visited_findings[linked_idx]:\n",
    "                            visited_findings[linked_idx] = True\n",
    "                            queue.append(linked_idx)\n",
    "\n",
    "            # 3. Construct VulnPair for this component\n",
    "            comp_files = set()\n",
    "\n",
    "            for f_idx in component_findings_indices:\n",
    "                comp_files.update(finding_files[f_idx])\n",
    "\n",
    "            # Read file contents and build affected_files dict {filename: content}\n",
    "            affected_files_dict = {}\n",
    "            for f_path in comp_files:\n",
    "                try:\n",
    "                    with open(f_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                    # Use relative path or just filename as key\n",
    "                    file_key = os.path.basename(f_path)\n",
    "                    # If there are duplicate filenames, use relative path from dataset root\n",
    "                    if file_key in affected_files_dict:\n",
    "                        try:\n",
    "                            file_key = str(Path(f_path).relative_to(dataset_path_abs))\n",
    "                        except ValueError:\n",
    "                            # If relative_to fails, use full path as fallback\n",
    "                            file_key = f_path\n",
    "                    affected_files_dict[file_key] = content\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not read file {f_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Generate global VFP ID\n",
    "            vfp_id = f\"vfp_{global_vfp_counter:05d}\"\n",
    "            global_vfp_counter += 1\n",
    "\n",
    "            vp = VulnerabilityFilePair(\n",
    "                vfp_id=vfp_id,\n",
    "                project_name=Path(report.path).name,\n",
    "                findings=[valid_findings[idx] for idx in component_findings_indices],\n",
    "                affected_files=affected_files_dict,\n",
    "            )\n",
    "            vuln_pairs.append(vp)\n",
    "\n",
    "            # Save VFP to file\n",
    "            vfp_output_path = output_vfp_dir / f\"{vfp_id}.json\"\n",
    "            try:\n",
    "                with open(vfp_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(vp.model_dump(), f, indent=4, ensure_ascii=False)\n",
    "                # print(f\"Saved VFP: {vfp_output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving VFP {vfp_id}: {e}\")\n",
    "\n",
    "    return vuln_pairs\n",
    "\n",
    "\n",
    "VFP_DIR = \"../flatten/vfp\"\n",
    "VFP_VULN_DIR = \"../flatten/vfp-vuln\"\n",
    "\n",
    "\n",
    "if ONLY_VULN:\n",
    "    for file in Path(VFP_VULN_DIR).glob(\"*.json\"):\n",
    "        try:\n",
    "            file.unlink()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not delete VFP vuln file {file}: {e}\")\n",
    "    process_reports(input_path=FINDING_PATH, output_vfp_dir=VFP_VULN_DIR)\n",
    "else:\n",
    "    for file in Path(VFP_DIR).glob(\"*.json\"):\n",
    "        try:\n",
    "            file.unlink()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not delete VFP file {file}: {e}\")\n",
    "    process_reports(input_path=FINDING_PATH, output_vfp_dir=VFP_DIR)\n",
    "\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b3d12",
   "metadata": {},
   "source": [
    "## Extract Only Solidity Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b90cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "contracts_raw_dir = Path(CONTRACTS_RAW_PATH)\n",
    "contracts_dir = Path(CONTRACTS_PATH)\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(contracts_raw_dir):\n",
    "    relative_path = os.path.relpath(root, contracts_raw_dir)\n",
    "    target_dir = contracts_dir / relative_path\n",
    "    for file in files:\n",
    "        if file.endswith(\".sol\"):\n",
    "            source_file = Path(root) / file\n",
    "            target_file = target_dir / file\n",
    "            target_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(source_file, target_file)\n",
    "            # logger.info(f\"Copied {source_file} to {target_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
